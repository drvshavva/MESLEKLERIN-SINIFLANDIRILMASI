# -*- coding: utf-8 -*-
"""b3lab_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g7LtpzxzDXN7z9DgaRtDAtr_29ubJk-B
iletisim :drvshavva@gmail.com
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

#temizlemek istedigimiz veri seti okuyoruz
data=pd.read_csv("saglik.csv")
data.head()

#kullanacagimiz ozellikler disindaki ozellikleri siliyoruz
data.drop(['calisma_sekli','egitim','sehir','sektor','sirket_adi','tecrube','departman'], axis=1,inplace=True)
data.head()

#veri setinin buyuklugu
len(data)

#veri seti ila ilgili bilgi ediniyoruz
data.info()

#stop words icin gereklikutuphaneleri indiriyoruz
!pip install TurkishStemmer

!pip install snowballstemmer

import re
import nltk
from nltk.tokenize import WordPunctTokenizer
from TurkishStemmer import TurkishStemmer
nltk.download('stopwords')
#from snowballstemmer import TurkishStemmer
stemmer = TurkishStemmer()
WPT = nltk.WordPunctTokenizer()
stop_word_list = nltk.corpus.stopwords.words('turkish')

#bu fonkiyonda:
#harfler kucuk harfe donusturuluyor, turkce karakter donusumu yapiliyor, noktalama vb karakterler siliniyor
#rakamlar kaldiriliyor, text kelimelerine ayrilip stop word olup olmadigi kontrol ediliyor eger degilse yeni olusturulan text kelimelerine eklenir
#son olarak kelimler aralarinde bir bosluk olacak sekilde birlestirilir
def clean(text):
    processed_tweet = []
    text=text.lower()
    
    text=text.replace('ş','s')
    text=text.replace('ı','i')
    text=text.replace('ö','o')
    text=text.replace('ü','u')
    text=text.replace('ğ','g')
    text=text.replace('ç','c')
   
    pattern = r"[{}]".format('&+#*·–’“•\'"?!,.():;><_/-') 
    text = re.sub(pattern, " ", text) 
  
    text = re.sub(r'[0-9]+', ' ', text)
    text = text.strip()
    #burada eger basta i varsa onu kelimeden ayriyor istanbul => i, stanbul
    #soralim stopwords kullanimini ve usteki
    tokens = WPT.tokenize(text) 
    filtered_tokens = [token for token in tokens if token  not in stop_word_list ]
    filtered_tokens = [token for token in tokens if len(token)>3 ]
    text = ' '.join(filtered_tokens)
    return text

#veri setinden bir cumle icin deneme yapiliyor
text = data['genel_nit_is_tanimi'][0]
text

text = clean(text)
text

#veri setindeki iki ozellik icin clean fonksiyonu cagriliyor
%%time
clean_is_tanim = []
clean_ilan_baslik = []
for i in range(0,len(data)):               
     clean_is_tanim.append(clean(data['genel_nit_is_tanimi'][i]))
     clean_ilan_baslik.append(clean(data['ilan_baslik'][i]))
#temizlendikten sonra veri setinin buyuklugu kontrol ediliyor
len(clean_ilan_baslik)

#yeni bir df olusturulup verilerin son hali csv dosyasina kaydediliyor
clean_df = pd.DataFrame(clean_is_tanim,columns=['genel_nit_is_tanimi'])
clean_df['ilan_baslik'] = clean_ilan_baslik
clean_df['label'] = data.label
clean_df.to_csv('saglik_cleaned.csv',encoding='utf-8')
clean_df.head()


